- make sure to verify the implimentation of AMP in the code is done correctly
- make sure to switch between model.eval mode and model.train mode accordingly

- flash attention is useful only for larger block sizes, and actully counterproductive for batch sizes less than 512