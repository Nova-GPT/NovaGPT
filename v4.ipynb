{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO RND - ARUSH - find a better way to validate current one just gives a number not very sure how good it is. also training parameter also seems a bit dicy\n",
    "    # sub-TODO - check if we our current implimentation is indiviually or semi-batch. Basically are we udpating the losses after accumilating a bunch of data or after ever run (shayadse called gradient accumilation)\n",
    "# TODO RND explore the concept of Progressive Context Increment : first block size is 64, than 128 then 256 and so on, for more efficient trainnig\n",
    "# TODO RND explore the paper that added another layer to decrease from quadratic training time to linear training time\n",
    "# TODO PRIORITY-3 - PRANAV - mixture of experts layer implimentation\n",
    "# TODO PRIORITY-4 - ARUSH - a grapher for the training process validation and test time\n",
    "# TODO PRIORITY-1 - ARUSH - a function to save the model weights whenever needed\n",
    "# TODO RND - DEVANSH - Explore Multi-Head Latent Attention (MLA) in DeepSeek-V2 and R1 and if it is relevent to us\n",
    "# TODO RND Expore various other innovations brought by deepseek ( reference : https://medium.com/@jannadikhemais/the-engineering-innovations-behind-deepseek-how-a-chinese-startup-redefined-ai-efficiency-90ea30788829 )\n",
    "# TODO RND Understand what are the various tricks used in SmolLM 1 to 3\n",
    "# TODO PRIORITY-1 - Implimentation of Automatic Mixed Precision(AMP) for 2x to 3x gains in speed and 50% memory usage allowing for larger batch sizes\n",
    "# TODO PRIORITY-3 - Impliment a tool to check the number of times a particular token exisits in a db and then sort and display a histogram with buckets as to how much of each token from vocab is there - to be able to check if each token as sufficient amount of vocab in the llm.\n",
    "# TODO PRIORITY-2 - Add a loading bar along with percent, which basically indicates how much dataset is covered.\n",
    "# TODO RND - A way to analyze the mode, such that to see which model weights have actually been trainned and which are reduntant, basically a way to analyize how useful each parameter is to the model\n",
    "\n",
    "# arush read RL deepseek\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1833966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from layers.RegexTokenizer import RegexTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from utils import ModelSpecs, TrainingData\n",
    "from layers.Block import Block\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "modelSpecs = ModelSpecs.create('small')\n",
    "# BATCH_SIZE = 64 # how many independent sequences will we process in parallel?\n",
    "BATCH_SIZE = 32 # how many independent sequences will we process in parallel?\n",
    "MAX_ITERS = 2000\n",
    "LEARNING_RATE = 3e-4\n",
    "EVAL_INTERVALS = 100\n",
    "# EVAL_ITERS = 200\n",
    "EVAL_ITERS = 50\n",
    "TRAIN_TEST_SPLIT = 0.9\n",
    "# device = 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Running on {device.capitalize()} device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "stories = TrainingData.TinyStories() # TODO impliment data caching in TrainingDataing Class so that we do not read the txt file again if already read once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "text = stories[int(10**6 * 0.730612 * i):int(10**6 * 0.730612 *(i+1) )]\n",
    "# text = TrainingData.TinyStories()[:int(10**6 * 0.730612 * 20 * 5)]\n",
    "print(len(text)/10**6,\"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba71be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = torch.tensor(tokenizer.encode(text))\n",
    "\n",
    "n = int(TRAIN_TEST_SPLIT * len(data))\n",
    "print(len(data)/10**6,\"M\")\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209488c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - modelSpecs.BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+modelSpecs.BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+modelSpecs.BLOCK_SIZE+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45497b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(modelSpecs.VOCAB_SIZE, modelSpecs.N_EMBD)\n",
    "        self.position_embedding_table = nn.Embedding(modelSpecs.BLOCK_SIZE, modelSpecs.N_EMBD)\n",
    "        self.blocks = nn.Sequential(*[Block(modelSpecs) for _ in range(modelSpecs.N_LAYER)])\n",
    "        self.ln_f = nn.LayerNorm(modelSpecs.N_EMBD) # final layer norm\n",
    "        self.lm_head = nn.Linear(modelSpecs.N_EMBD, modelSpecs.VOCAB_SIZE)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -modelSpecs.BLOCK_SIZE:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b945baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model : GPTLanguageModel):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "    print(f\"iter #{iter}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % EVAL_INTERVALS == 0 or iter == MAX_ITERS - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {int((time.time() - startTime)//60)} minutes\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "endTime = time.time()\n",
    "print(f\"Total Training Time : {int((endTime - startTime)//60)} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2434cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = torch.tensor(tokenizer.encode(\"there was once a forest with 100s of bushes\")).unsqueeze(0).cuda()\n",
    "# print(input_tokens)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "output_tokens = m.generate(input_tokens, max_new_tokens=64)[0]\n",
    "# print(output_tokens)\n",
    "\n",
    "model.train()\n",
    "\n",
    "output : str = tokenizer.decode(output_tokens)\n",
    "\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3aaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int(len(stories)//(10**6 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 100\n",
    "startTime = time.time()\n",
    "step = BATCH_SIZE * modelSpecs.BLOCK_SIZE * ITERS * 8\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "# for i in range(step * 0, len(stories), step):\n",
    "for i in range(989593600, len(stories), step):\n",
    "    print(\"ITER:\", i // step, \"::::\", \" STRING INDEX:\", i)\n",
    "    text = stories[i : i + step]\n",
    "    data = torch.tensor(tokenizer.encode(text))\n",
    "    n = int(TRAIN_TEST_SPLIT * len(data))\n",
    "    print(\"tokens\", len(data)/10**6,\"M\")\n",
    "\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for iter in range(ITERS):\n",
    "        print(f\"iter #{iter}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % EVAL_INTERVALS == 0 or iter == ITERS - 1:\n",
    "            losses = estimate_loss(model)\n",
    "            liveloss.update({ 'loss': losses['train'], 'val_loss': losses['val']})\n",
    "            liveloss.send()\n",
    "            print(\"ITER:\", i // step, \"::::\", \" STRING INDEX:\", i)\n",
    "            print(\"tokens\", len(data)/10**6,\"M\")\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {int((time.time() - startTime)//60)} minutes\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    endTime = time.time()\n",
    "    print(f\"Total Training Time : {int((endTime - startTime)//60)} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4da161",
   "metadata": {},
   "outputs": [],
   "source": [
    "655,360,000\n",
    "989,593,600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = torch.tensor(tokenizer.encode(\" \")).unsqueeze(0).cuda()\n",
    "# print(input_tokens)\n",
    "\n",
    "\n",
    "output_tokens = model.generate(input_tokens, max_new_tokens=100)[0]\n",
    "# print(output_tokens)\n",
    "\n",
    "output : str = tokenizer.decode(output_tokens)\n",
    "\n",
    "\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_optimizer_state_to_float32(optimizer : torch.optim.AdamW):\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor) and v.dtype == torch.float64:\n",
    "                state[k] = v.float()\n",
    "\n",
    "\n",
    "# Saving the model\n",
    "def save_model(model : nn.Module, optimizer : torch.optim.AdamW, filepath):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "# Loading the model\n",
    "def load_model(model : nn.Module, optimizer : torch.optim.AdamW, filepath, device):\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    convert_optimizer_state_to_float32(optimizer)\n",
    "\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    model.to(device)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a86e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save after training\n",
    "save_model(model, optimizer, 'weights/gpt_model_50M.pth') # TODO improve this naming convention\n",
    "\n",
    "# Later or for inference\n",
    "# model = GPTLanguageModel()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# model, optimizer = load_model(model, optimizer, 'gpt_model_checkpoint.pth', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = GPTLanguageModel()\n",
    "pre_optimizer = torch.optim.AdamW(pre_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loaded_model, loaded_optimizer = load_model(pre_model, pre_optimizer, './weights/gpt_model_50M.pth', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a520c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = loaded_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deab6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 100\n",
    "startTime = time.time()\n",
    "step = BATCH_SIZE * modelSpecs.BLOCK_SIZE * ITERS * 8\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "# for i in range(step * 0, len(stories), step):\n",
    "for i in range(989593600, len(stories), step):\n",
    "    print(\"ITER:\", i // step, \"::::\", \" STRING INDEX:\", i)\n",
    "    text = stories[i : i + step]\n",
    "    data = torch.tensor(tokenizer.encode(text))\n",
    "    n = int(TRAIN_TEST_SPLIT * len(data))\n",
    "    print(\"tokens\", len(data)/10**6,\"M\")\n",
    "\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for iter in range(ITERS):\n",
    "        print(f\"iter #{iter}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % EVAL_INTERVALS == 0 or iter == ITERS - 1:\n",
    "            losses = estimate_loss(loaded_model)\n",
    "            liveloss.update({ 'loss': losses['train'], 'val_loss': losses['val']})\n",
    "            liveloss.send()\n",
    "            print(\"ITER:\", i // step, \"::::\", \" STRING INDEX:\", i)\n",
    "            print(\"tokens\", len(data)/10**6,\"M\")\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {int((time.time() - startTime)//60)} minutes\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = loaded_model(xb, yb)\n",
    "        loaded_optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        loaded_optimizer.step()\n",
    "\n",
    "    endTime = time.time()\n",
    "    print(f\"Total Training Time : {int((endTime - startTime)//60)} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbba3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = torch.tensor(tokenizer.encode(\" \")).unsqueeze(0).cuda()\n",
    "# print(input_tokens)\n",
    "\n",
    "\n",
    "output_tokens = loaded_model.generate(input_tokens, max_new_tokens=100)[0]\n",
    "# print(output_tokens)\n",
    "\n",
    "output : str = tokenizer.decode(output_tokens)\n",
    "\n",
    "\n",
    "\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
